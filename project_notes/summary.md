ä¸‹é¢æ˜¯ä¸€ä»½**æ¸…æ™°ã€ç»“æ„åŒ–ã€å¯ç›´æ¥ä½œä¸ºé¡¹ç›®å¯¹é½æ–‡æ¡£/è¿›å±•æ€»ç»“çš„â€œé¡¹ç›®æ€»ç»“â€**ã€‚
å®ƒè¦†ç›–äº†ä½ ç›®å‰å·²ç»ç»Ÿä¸€çš„æ€æƒ³ã€æŠ€æœ¯è·¯çº¿ã€Phase 0 çš„å…·ä½“å®ç°æ–¹å‘ã€æ•°æ®ç»“æ„æŠ½è±¡ã€ä»¥åŠä¸ºä½•è¿™æ ·è®¾è®¡çš„é€»è¾‘ä¾æ®ã€‚

---

# ğŸš€ **Memory Agent RL Framework â€”â€” é¡¹ç›®æ€»ç»“ï¼ˆæ€»è§ˆç‰ˆï¼‰**

æœ¬é¡¹ç›®æ—¨åœ¨æ„å»ºä¸€ä¸ª **ç»Ÿä¸€çš„è®°å¿†å¢å¼ºå‹ LLM å¼ºåŒ–å­¦ä¹ ç³»ç»Ÿ**ï¼Œèƒ½å¤ŸåŒæ—¶é€‚é…ï¼š

* **é•¿æ–‡æ¡£è®°å¿†ä»»åŠ¡ï¼ˆMem-Î± èŒƒå¼ï¼‰**
* **å¢é‡å¼å¤šè½®å¯¹è¯è®°å¿†ä»»åŠ¡ï¼ˆMemoryAgentBenchï¼ŒHu 2025ï¼‰**
* **å¸¦åé¦ˆçš„æŒç»­å­¦ä¹  / test-time learningï¼ˆMemoryBenchï¼ŒAi 2025ï¼‰**

æ ¸å¿ƒç›®æ ‡æ˜¯ï¼š

> **æ„å»ºä¸€ä¸ªç»Ÿä¸€çš„ multi-turn RL rollout pipelineï¼Œè®©æ‰€æœ‰è®°å¿†ç±» benchmark éƒ½èƒ½åœ¨åŒä¸€æ¡†æ¶ä¸‹è®­ç»ƒã€‚**

---

# ğŸ§  1. é¡¹ç›®å½“å‰è¾¾æˆçš„å…³é”®å…±è¯†ï¼ˆCritical Insightsï¼‰

## **1.1 Multi-turn RL æ˜¯å”¯ä¸€èƒ½ç»Ÿä¸€æ‰€æœ‰è®°å¿†ä»»åŠ¡çš„å…±äº«æ¡†æ¶**

ä¸‰ä¸ª benchmark å½¢å¼å„ä¸ç›¸åŒï¼š

| Benchmark        | åŸå§‹å½¢å¼          | å…³é”®è¡Œä¸º                    |
| ---------------- | ------------- | ----------------------- |
| Mem-Î±            | chunk åºåˆ— + QA | é€ chunk æ›´æ–° memory       |
| MemoryAgentBench | å¤šè½®å¯¹è¯          | å¢é‡æ›´æ–°ã€é•¿æœŸè®°å¿†               |
| MemoryBench      | feedback logs | test-time learningã€å¤šè½®åé¦ˆ |

ä½†å®ƒä»¬çš„å…±åŒç»“æ„å´æ˜¯å®Œå…¨ä¸€è‡´çš„ï¼š

> **æ¯ä¸€è½®è¾“å…¥ä¸€æ®µæ–°ä¿¡æ¯ â†’ æ¨¡å‹æ›´æ–° internal memory â†’ ä¸‹ä¸€è½®ç»§ç»­ã€‚**

å› æ­¤é¡¹ç›®å°†å®ƒä»¬å…¨éƒ¨è½¬åŒ–ä¸ºï¼š

```
Turn 1: obs_1 â†’ action_1 â†’ memory_1
Turn 2: obs_2 â†’ action_2 â†’ memory_2
...
Turn T: obs_T (query) â†’ action_T (answer) â†’ reward
```

è¿™ä½¿ä½ å¯ä»¥ï¼š

* ä½¿ç”¨ **ç»Ÿä¸€çš„ rollout pipeline**
* åŸºäº GRPO/PPO è‡ªåŠ¨ä¼˜åŒ–åºåˆ—å†³ç­–
* ç›´æ¥æ‰©å±• memory æ¨¡å—è€Œä¸æ”¹å˜æ•´ä½“æ¡†æ¶

---

## **1.2 Mem-Î± æœ¬è´¨ä¸Šæ˜¯ multi-step RLï¼Œè€ŒéçœŸæ­£çš„ multi-turn ä¼šè¯**

æˆ‘ä»¬å·²æ˜ç¡®ï¼š

* Mem-Î± æ¯ä¸ª chunk éƒ½ä½œä¸ºä¸€æ­¥ state
* æ¨¡å‹ç”Ÿæˆ memory operations æ–‡æœ¬
* python åº”ç”¨ ops æ›´æ–° memory
* ç»§ç»­ä¸‹ä¸€ä¸ª chunk

è™½ç„¶æ²¡æœ‰èŠå¤©æ ¼å¼ï¼Œä½†**ä» RL è§†è§’å°±æ˜¯ multi-turn MDP**ã€‚

æ‰€ä»¥ï¼š

> **Mem-Î± å¯ä»¥è‡ªç„¶æ˜ å°„åˆ° multi-turn rolloutï¼Œä¸éœ€è¦ç»´æŒå…¶åŸç”Ÿâ€œå•è½®å¤§ promptâ€å½¢å¼ã€‚**

è¿™ä¸ºç»Ÿä¸€æ•°æ®ç»“æ„æä¾›äº†ç†è®ºåŸºç¡€ã€‚

---

## **1.3 MemoryAgentBench å’Œ MemoryBench å¤©ç„¶æ˜¯ multi-turn**

* MemoryAgentBenchï¼å¤šè½®å¯¹è¯ + æœ€ç»ˆé—®ç­”
* MemoryBenchï¼ˆLoCoMo ç­‰ï¼‰ï¼äº¤äº’å¼åé¦ˆ
* MemoryBench å…¶å®ƒä»»åŠ¡ä¹Ÿæœ‰æ˜ç¡®çš„ â€œfeedback â†’ correction â†’ next stepâ€

å› æ­¤ä½ çš„ multi-turn æŠ½è±¡å®Œå…¨è´´åˆä»»åŠ¡ç‰¹æ€§ã€‚

---

## **1.4 Phase 0 å¿…é¡»ä»¥ multi-turn rollout ä¸ºæ ¸å¿ƒ**

ä¸ºäº†è®© Phase 1/2/3 çš„è®°å¿†æ¨¡å—é¡ºåˆ©æ¥å…¥ï¼š

* Phase 0 çš„ rollout pipeline å¿…é¡»æ”¯æŒå¤šè½®äº¤äº’
* æ¯ä¸€è½® action å¿…é¡»è¿”å›åˆ°ç¯å¢ƒä¸­äº§ç”Ÿä¸‹ä¸€è½® obs
* rollout ç»“æŸæä¾› full trajectory ç»™ GRPO

è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿçš„åœ°åŸºã€‚

---

# ğŸ— 2. é¡¹ç›®ç»Ÿä¸€è®¾è®¡æ€æƒ³ï¼ˆUnified Design Principleï¼‰

æ ¸å¿ƒè®¾è®¡åŸåˆ™ï¼š

> **å°†æ‰€æœ‰ benchmark ç»Ÿä¸€è½¬æ¢ä¸º multi-turn environment â†’ RL rollout â†’ trajectory sampleã€‚**

è½¬æ¢è§„åˆ™ï¼š

| Benchmark        | Multi-turn è½¬æ¢æ–¹å¼                                 |
| ---------------- | ----------------------------------------------- |
| Mem-Î±            | turn_t = (memory_{t-1}, chunk_t)                |
| MemoryAgentBench | turn_t = (memory_{t-1}, user_utterance_t)       |
| MemoryBench      | turn_t = (memory_{t-1}, feedback_t / context_t) |

æœ€ç»ˆç»Ÿä¸€ä¸ºï¼š

```
Env.reset() â†’ memory_0
for t in range(T):
    obs_t = Env.get_turn(t, memory_t)
    action_t = Actor.generate(obs_t)
    memory_{t+1} = MemoryModule.update(memory_t, action_t)
reward = evaluate(final_answer, ground_truth)
```

---

# ğŸ§± 3. å½“å‰å®æ–½è·¯å¾„ï¼ˆPhasesï¼‰

## **Phase 0ï¼šæ„å»º Multi-turn RL Rolloutï¼ˆä½ æ­£åœ¨åšï¼‰**

ç›®æ ‡ï¼š

* vLLM + LoRA + VeRL å®ç°è‡ªå®šä¹‰ rolloutï¼ˆæ”¯æŒå¤šè½® generateï¼‰
* å°†å¤šä¸ª benchmark è½¬æˆç»Ÿä¸€ multi-turn æ ¼å¼
* èƒ½è®­ç»ƒä¸€ä¸ªç®€å• policyï¼ˆä¸ç”¨ memoryï¼‰

å…³é”®æˆæœï¼š

* ç¡®ç«‹äº† multi-turn æ•°æ®ç»“æ„
* å®šä¹‰äº†ç»Ÿä¸€çš„ç¯å¢ƒæ¥å£ï¼ˆobs â†’ action â†’ next_obsï¼‰
* æ‰€æœ‰ benchmark å‡å¯ä½œä¸º multi-turn episodes é©±åŠ¨ rollout
* æŠ€æœ¯è·¯çº¿ä¸Šå·²æ˜ç¡®ï¼šrollout å¿…é¡»è‡ªå·±å†™ï¼Œä¸ä¾èµ– VeRL AgentLoop

## **Phase 1ï¼šåŠ å…¥éšå¼ Memoryï¼ˆMem-Î± ç±»ï¼‰**

* Memory = dict / structured object
* æ¯è½® action = ç»“æ„åŒ– opsï¼ˆADD/UPDATE/DELETEï¼‰æˆ–éšå¼å†…å­˜å‹ç¼©
* Memory ä¸ç›´æ¥ä½œä¸º promptï¼Œè€Œæ˜¯é‡æ–°æ¸²æŸ“ä¸ºâ€œMemory State Promptâ€
* æœ€ç»ˆ answer ä¹Ÿå¯åƒ Mem-Î± é‚£æ ·äº¤ç»™ frozen LLM

## **Phase 2ï¼šåŠ å…¥æ˜¾å¼ Memory Managerï¼ˆMemory-R1 / Mem0 ç±»ï¼‰**

* ä¸“é—¨çš„ Memory Manager agent
* Answer agentï¼ˆæˆ–å…±äº«ï¼‰
* å­¦ä¹ çœŸæ­£çš„é•¿æœŸè®°å¿†ç®¡ç†ç­–ç•¥

## **Phase 3ï¼šå¤š Agent åä½œï¼ˆäº‹å®æ‹†åˆ† agent / é•¿çŸ­æœŸè®°å¿† agentï¼‰**

* äº‹å®æ‹†åˆ†ï¼ˆimportant / unimportantï¼‰
* é•¿æœŸè®°å¿† agentï¼ˆADD/UPDATE/DELETEï¼‰
* çŸ­æœŸè®°å¿† agentï¼ˆstate summarizationï¼‰
* ç”Ÿæˆ agentï¼ˆå›ç­” queryï¼‰

å®Œå…¨å¯¹åº”ä½ åœ¨ Memory.md ä¸­çš„è®¾è®¡ã€‚

---

# ğŸ“¦ 4. å½“å‰æ•°æ®æµï¼ˆFinal Unified Data Flowï¼‰

## 1. æ•´ä½“è°ƒç”¨æµç¨‹ & æ•°æ®æµï¼ˆfrom RayPPOTrainer åˆ° GRPOï¼‰

å…ˆç”¨ä¸€ä¸ªâ€œé¸Ÿç°å›¾â€çœ‹æ•´ä½“ï¼š

```text
RayPPOTrainer.fit()
  â”œâ”€ è¯» dataloader â†’ å¾—åˆ°ä¸€ä¸ª batch_dialogues (B æ¡å¯¹è¯æ ·æœ¬)
  â”œâ”€ å¯¹æ¯æ¡æ ·æœ¬ iï¼Œç”Ÿæˆ G æ¡ rolloutï¼š
  â”‚     â”œâ”€ è°ƒç”¨ dialogue_runner.run_episode(...)
  â”‚     â”‚     â”œâ”€ å†…éƒ¨è·‘å¤šè½®å¯¹è¯ï¼š
  â”‚     â”‚     â”‚     â”œâ”€ æ¯ä¸ª turn è°ƒæ¨¡å‹ä¸€æ¬¡æˆ–å¤šæ¬¡ï¼ˆè®°å¿†æ›´æ–° + å›ç­”ï¼‰
  â”‚     â”‚     â”‚     â””â”€ æ”¶é›†æˆ EpisodeTrajectoryï¼ˆtoken, logprob, mask, metaï¼‰
  â”‚     â”‚     â””â”€ è¿”å› (episode_traj_i_j, scalar_reward_i_j)
  â”‚     â””â”€ è®°å½• group_id = æ ·æœ¬ i çš„ idï¼Œç”¨äº GRPO åˆ†ç»„
  â”œâ”€ å°†æ‰€æœ‰ EpisodeTrajectory + reward + group_id
  â”‚   flatten & pad â†’ ç»Ÿä¸€æˆè‹¥å¹² tensor
  â”‚   â†’ å°è£…æˆ DataProto
  â””â”€ è°ƒç”¨ Verl çš„ core_algo.update(data_proto)
          â””â”€ å†…éƒ¨åš GRPO / PPOï¼Œæ›´æ–° LoRA å‚æ•°
```

å…³é”®ç‚¹ï¼š

* **ä¸€ä¸ª episode = ä¸€æ•´æ¡å¯¹è¯è½¨è¿¹**ï¼ˆåŒ…å«å¤šè½® memory ops + å›å¤ï¼‰
* **ä¸€ä¸ª group = åŒä¸€æ¡è¾“å…¥å¯¹è¯ï¼Œåœ¨åŒä¸€ step ç”Ÿæˆçš„å¤šæ¡ä¸åŒ rollout**ï¼ˆå¤šæ¬¡é‡‡æ ·ï¼‰
* Verl çš„ GRPO åªçœ‹åˆ°ï¼š

  * ä¸€æ‰¹ token åºåˆ— + å¯¹åº” logprobsã€response_mask
  * ä¸€ä¸ªæŒ‰ group åˆ†ç»„çš„ reward å‘é‡

---

## 2. EpisodeTrajectory / StepTrajectory çš„ç»“æ„è®¾è®¡

### 2.1 StepTrajectoryï¼šè®°å½•ã€Œæœ¬æ¬¡æ¨¡å‹è°ƒç”¨ã€çš„ä¿¡æ¯

æ¯æ¬¡ä½ è°ƒä¸€æ¬¡ vLLMï¼ˆæ— è®ºæ˜¯ memory ops æ­¥éª¤è¿˜æ˜¯ answer æ­¥éª¤ï¼‰ï¼Œå…¶å®å°±æ˜¯ä¸€æ¡â€œå°è½¨è¿¹ stepâ€ã€‚æˆ‘ä»¬å¯ä»¥å®šä¹‰ï¼š

```python
from dataclasses import dataclass, field
from typing import List, Dict, Any
import torch

@dataclass
class StepTrajectory:
    # å•æ¬¡ generation äº§ç”Ÿçš„ token åºåˆ—
    input_ids: torch.LongTensor      # shape: [seq_len]
    attention_mask: torch.LongTensor # shape: [seq_len]
    position_ids: torch.LongTensor   # shape: [seq_len]ï¼ˆå¯é€‰ï¼Œçœ‹ä½ æ˜¯å¦è¦æ‰‹åŠ¨ä¼ ï¼‰
    
    # æ¨¡å‹è¾“å‡ºçš„ logprobsï¼ˆactor æ¨¡å‹ï¼‰ï¼Œå’Œ Verl ä¸€è‡´ï¼š
    logprobs: torch.FloatTensor      # shape: [seq_len]
    # å“ªäº› token æ˜¯ã€Œæ¨¡å‹ç”Ÿæˆçš„ã€ï¼ˆç”¨äºè®¡ç®— loss / advantageï¼‰
    response_mask: torch.BoolTensor  # shape: [seq_len]
    
    # å¯é€‰ï¼šå·¥å…·è°ƒç”¨ä¸å…¶ä»–å…ƒä¿¡æ¯
    tool_calls: List[Dict[str, Any]] = field(default_factory=list)
    # å¯é€‰ï¼šå¦‚æœä½ æœ‰ per-step å·¥å…· rewardï¼Œå¯ä»¥å…ˆè®°åœ¨è¿™
    step_reward: float = 0.0
```

**è¯´æ˜ï¼š**

* `input_ids` / `attention_mask` / `position_ids` å¯ä»¥ç›´æ¥ä» vLLM rollout æ‹¿å›æ¥ï¼š

  * prompt + generated ä¸€æ•´ä¸ªæ‹¼åœ¨ä¸€èµ·ï¼›
  * `response_mask` ç”¨æ¥æ ‡è®°ã€Œå“ªäº› token å±äºæ¨¡å‹ç”Ÿæˆéƒ¨åˆ†ã€ï¼ˆæ¯”å¦‚ prompt = 0ï¼Œç”Ÿæˆéƒ¨åˆ† = 1ï¼‰ã€‚
* `logprobs` æ˜¯ Verl ç®— policy loss å¿…é¡»çš„ï¼›
* `tool_calls` æ˜¯ä½ è‡ªå·±åç»­åˆ†æç”¨ï¼Œå¯ä»¥ä¸è¿› DataProtoã€‚

### 2.2 EpisodeTrajectoryï¼šä¸€æ¡å¯¹è¯ episode çš„æ‰€æœ‰ step

```python
@dataclass
class EpisodeTrajectory:
    # æŠŠæ¯ä¸€æ¬¡æ¨¡å‹è°ƒç”¨å¯¹åº”çš„ step éƒ½å­˜è¿›æ¥
    steps: List[StepTrajectory] = field(default_factory=list)

    # å…ƒä¿¡æ¯ï¼šè¿™æ¡ episode å±äºå“ªä¸ªæ ·æœ¬ / å“ªä¸ª groupï¼ˆGRPOï¼‰
    episode_id: int = -1          # ä½ å†…éƒ¨ç”¨çš„ç´¢å¼•
    group_id: int = -1            # = æŸæ¡è¾“å…¥å¯¹è¯çš„ idï¼ˆå¯¹äº GRPOï¼Œä¸€ä¸ª group å¯¹åº”å¤šæ¡ episodeï¼‰
    
    # æ•´æ¡ episode çš„ scalar rewardï¼ˆfinalï¼‰
    reward: float = 0.0
```

> åœ¨ `run_episode` ç»“æŸæ—¶ï¼Œä½ å°±è¿”å›ä¸€ä¸ª `EpisodeTrajectory`ï¼Œé‡Œé¢åŒ…å«è¿™æ¡å¯¹è¯å…¨è¿‡ç¨‹æ‰€æœ‰ step çš„ token + logprobs + maskï¼›
> ç„¶åä½ åœ¨è®­ç»ƒä¸»å¾ªç¯é‡Œä¼šæœ‰å¾ˆå¤š episodeï¼ŒæŠŠå®ƒä»¬æ‰“åŒ…æˆæ‰¹æ¬¡å–‚ç»™ Verlã€‚

---

## 3. ä» episode åˆ—è¡¨ â†’ pad æˆ batch tensor çš„æµç¨‹

### 3.1 å±•å¹³ & è®¡ç®— batch å°ºåº¦

å‡è®¾è¿™ä¸€è½®è®­ç»ƒä½ æ€»å…±ç”Ÿæˆäº†ï¼š

* `N_episodes` æ¡ episodeï¼ˆ= `B Ã— G`ï¼‰
* æ¯æ¡ episode æœ‰ `S_i` ä¸ª stepï¼Œæ¯ä¸ª step é‡Œæœ‰ `L_{i,s}` ä¸ª token

ä¸ºäº†å–‚ç»™ Verlï¼Œä½ éœ€è¦æŠŠå®ƒä»¬å˜æˆç±»ä¼¼ï¼š

* `input_ids`: `[N_total, max_seq_len]`
* `logprobs`: `[N_total, max_seq_len]`
* `response_mask`: `[N_total, max_seq_len]`

è¿™é‡Œ `N_total` å¯ä»¥æœ‰ä¸¤ä¸ªé€‰æ‹©ï¼š

1. **æŒ‰ step ç»´åº¦å±•å¼€ï¼ˆæ¨èï¼Œç®€å•ï¼‰ï¼š**

   * æŠŠæ‰€æœ‰ episode çš„æ‰€æœ‰ step **æŒ‰é¡ºåºæ‹¼æˆä¸€ä¸ªé•¿åˆ—è¡¨**ï¼š

     * `flat_steps = [step for ep in episodes for step in ep.steps]`
   * é‚£ `N_total = Î£_i S_i`ï¼Œæ¯ row å¯¹åº”ã€Œä¸€æ¬¡æ¨¡å‹è°ƒç”¨çš„ä¸€æ•´ä¸ªåºåˆ—ã€ï¼›
   * æ¯ä¸ª row ä»ç„¶å¯ä»¥é€šè¿‡é¢å¤–ç´¢å¼•æ˜ å°„å› episode / group_idã€‚

2. **æŒ‰ episode å±•å¼€ï¼ŒæŠŠæ‰€æœ‰ step æ‹¼æˆä¸€æ¡é•¿åºåˆ—ï¼ˆæ›´å¤æ‚ï¼‰ï¼š**

   * å¯¹æ¯ä¸ª episodeï¼ŒæŠŠæ‰€æœ‰ step çš„ token concat æˆä¸€ä¸ªé•¿åºåˆ—ï¼›
   * `N_total = N_episodes`ï¼›
   * éœ€è¦è‡ªå·±å¤„ç†å¥½æ¯”å¦‚ä¸åŒ step é—´çš„åˆ†æ®µä¿¡æ¯ï¼›
   * å¯¹ Verl æ¥è¯´æ²¡å·®ï¼Œä½†ä½  debug è¾ƒéš¾çœ‹ã€‚

**å»ºè®®**ï¼š
å…ˆç”¨æ–¹æ¡ˆ 1ï¼ˆstep-level flattenï¼‰ï¼Œå®ç°ç®€å•ï¼Œè€Œä¸”ç›´è§‚å¯¹åº”ä½ çš„ã€Œæ¯æ¬¡è°ƒç”¨ vLLM ä¸€æ¡åºåˆ—ã€ã€‚

### 3.2 å…·ä½“çš„ pack å‡½æ•°ä¼ªä»£ç 

```python
def pack_episodes_to_batch_tensors(episodes: List[EpisodeTrajectory]):
    # 1. å±•å¹³æ‰€æœ‰ step
    flat_steps = []
    episode_idx_of_step = []
    group_id_of_step = []

    for ep_idx, ep in enumerate(episodes):
        for step in ep.steps:
            flat_steps.append(step)
            episode_idx_of_step.append(ep_idx)
            group_id_of_step.append(ep.group_id)

    num_steps = len(flat_steps)

    # 2. æ‰¾å‡ºè¿™ä¸€ä¸ª batch ä¸­ max_seq_len
    seq_lens = [s.input_ids.shape[0] for s in flat_steps]
    max_seq_len = max(seq_lens)

    # 3. åˆ†é… tensor
    # è¿™é‡Œç”¨ torch.zeros + å¡«å……ï¼ŒPad çš„éƒ¨åˆ†æ³¨æ„åœ¨ mask ä¸­ç½® 0
    input_ids      = torch.full((num_steps, max_seq_len), fill_value=pad_token_id, dtype=torch.long)
    attention_mask = torch.zeros((num_steps, max_seq_len), dtype=torch.bool)
    position_ids   = torch.zeros((num_steps, max_seq_len), dtype=torch.long)
    logprobs       = torch.zeros((num_steps, max_seq_len), dtype=torch.float32)
    response_mask  = torch.zeros((num_steps, max_seq_len), dtype=torch.bool)

    # 4. æŠŠæ¯æ¡ step çš„åºåˆ—æ‹·è¿›å»
    for i, step in enumerate(flat_steps):
        L = step.input_ids.shape[0]
        input_ids[i, :L]      = step.input_ids
        attention_mask[i, :L] = step.attention_mask
        position_ids[i, :L]   = step.position_ids
        logprobs[i, :L]       = step.logprobs
        response_mask[i, :L]  = step.response_mask

    # 5. æ„é€  reward / group_id å‘é‡ï¼ˆepisode ç²’åº¦ â†’ step ç²’åº¦ï¼‰
    #   - å¯¹ GRPO æ¥è¯´ï¼Œreward æ˜¯åœ¨ group å†…å½’ä¸€åŒ–ç”¨çš„ï¼Œ
    #     ä½ å¯ä»¥å…ˆæ„é€  episode-level rewardï¼Œç„¶åå¹¿æ’­åˆ°æ¯ä¸ª step
    episode_rewards = torch.tensor([ep.reward for ep in episodes], dtype=torch.float32)
    rewards = torch.zeros(num_steps, dtype=torch.float32)
    for i, ep_idx in enumerate(episode_idx_of_step):
        rewards[i] = episode_rewards[ep_idx]

    group_ids = torch.tensor(group_id_of_step, dtype=torch.long)

    return {
        "input_ids": input_ids,
        "attention_mask": attention_mask,
        "position_ids": position_ids,
        "logprobs": logprobs,
        "response_mask": response_mask,
        "rewards": rewards,
        "group_ids": group_ids,
        "episode_ids": torch.tensor(episode_idx_of_step, dtype=torch.long),
    }
```

> è¿™é‡Œçš„ `episode_ids` / `group_ids` ä¸»è¦æ˜¯ä¸ºäº† GRPO / ä½ è‡ªå·±çš„ç›‘æ§ç»Ÿè®¡è€Œå‡†å¤‡ï¼ŒVerl å†…éƒ¨å¯èƒ½åªéœ€è¦ group ç»´åº¦åš advantage å½’ä¸€åŒ–ï¼›
> ä½ å¯ä»¥æ ¹æ® Verl çš„å…·ä½“å®ç°é€‰æ‹©ç•™å“ªäº›å­—æ®µã€‚

---

## 4. pack æˆ Verl çš„ DataProto çš„å­—æ®µè®¾è®¡ï¼ˆæ¦‚å¿µç‰ˆï¼‰

Verl çš„ `DataProto` æœ¬è´¨å°±æ˜¯ä¸€ä¸ªå°è£…äº†è‹¥å¹² named tensors çš„ç»“æ„ï¼Œç±»ä¼¼ï¼š

```python
# ä¼ªä»£ç ï¼Œä»…è¯´æ˜å­—æ®µå«ä¹‰
data_proto = DataProto(
    input_ids=input_ids,                 # [num_steps, max_seq_len]
    attention_mask=attention_mask,       # [num_steps, max_seq_len]
    position_ids=position_ids,           # å¯é€‰
    logprobs=logprobs,                   # actor æ¨¡å‹ç”Ÿæˆæ—¶è®°å½•çš„ logÏ€(a|s)
    response_mask=response_mask,         # åªå¯¹è¿™äº› token è®¡ç®— loss
    reward=rewards,                      # per-step å…±äº«çš„ episode scalar reward
    group_ids=group_ids,                 # æ–¹ä¾¿ GRPO åœ¨ group ç»´åº¦åš mean/std
    # å¯é€‰ï¼šå¦‚æœ Verl çš„ KL éœ€è¦ ref_logprobsï¼Œå¯ä»¥å•ç‹¬å†è®© ref æ¨¡å‹è·‘ä¸€é
)
```

**æ ¸å¿ƒè¦ç‚¹ï¼š**

1. **input_ids / attention_mask / position_ids**

   * å’Œæ™®é€š Verl RL ä»»åŠ¡ä¸€æ ·ï¼Œåªæ˜¯æ¯è¡Œå¯¹åº”çš„æ˜¯ã€Œä¸€æ¬¡æ¨¡å‹è°ƒç”¨çš„åºåˆ—ï¼ˆä¸€ä¸ª stepï¼‰ã€ï¼›
   * å¯¹ Verl çš„ rollout/actor æ¥è¯´ï¼Œç»“æ„ä¸€æ ·ï¼Œåªæ˜¯ä½ æ˜¯åœ¨å¤–é¢å…ˆè·‘äº† agentic ç¯ã€‚

2. **logprobs / response_mask**

   * `logprobs` æ˜¯ actor åœ¨ rollout æ—¶ç®—å‡ºæ¥çš„ logÏ€(a|s)ï¼Œä½ åœ¨ EpisodeTrajectory é‡Œå­˜ç€ï¼›
   * `response_mask` å†³å®šå“ªé‡Œç®— RL lossï¼ˆprompt token é€šå¸¸ mask=0ï¼Œå›ç­”éƒ¨åˆ†=1ï¼‰ã€‚

3. **rewards**

   * è¿™é‡Œæˆ‘ä»¬é‡‡ç”¨ã€Œepisode scalar reward â†’ broadcast åˆ°æ‰€æœ‰ step çš„æ‰€æœ‰ tokenã€ï¼›
   * `rewards` æ˜¯ `[num_steps]` çš„å‘é‡ï¼›åœ¨ä¼˜åŒ–æ—¶ï¼Œä¼šæ ¹æ® `group_ids` èšåˆ/å½’ä¸€ã€‚

4. **group_ids**ï¼ˆGRPO å¿…å¤‡ï¼‰

   * åŒä¸€æ¡è¾“å…¥å¯¹è¯ï¼ˆåŒæ ·çš„ dialogue sampleï¼‰ç”Ÿæˆå¤šæ¡ rollout æ—¶ï¼Œè¿™äº› rollout çš„æ‰€æœ‰ step åº”è¯¥å…±äº«ä¸€ä¸ª group_idï¼›
   * GRPO ä¼šæŒ‰ group ç»´åº¦åš `R_i - mean(R)`ï¼Œå®ç°â€œç›¸å¯¹ä¼˜åŠ¿â€ã€‚

5. **ref_logprobs**ï¼ˆå¦‚æœéœ€è¦ KL penaltyï¼‰

   * Verl å†…éƒ¨å¾€å¾€ä¼šæœ‰ä¸€ä¸ª reference policyï¼ˆbase æ¨¡å‹ + å†»ç»“å‚æ•°ï¼‰ï¼›
   * ä½ å¯ä»¥ç”¨å®ƒçš„ rollout æˆ–ç¦»çº¿è°ƒç”¨ï¼Œç®—å‡º ref_logprobsï¼›
   * åŒæ · pack åˆ° DataProto é‡Œï¼Œç”¨äº KL termã€‚

---

## 5. å…¨æµç¨‹å†ä¸²ä¸€ä¸‹ï¼ˆä½ è„‘ä¸­è¦æœ‰çš„ã€Œæ•°æ®ç®¡é“å›¾ã€ï¼‰

ä»é«˜åˆ°ä½ä¸²ä¸€éï¼ˆè¿™éƒ¨åˆ†ä½ å¯ä»¥æƒ³è±¡æˆè„‘ä¸­æµç¨‹å›¾ï¼‰ï¼š

1. **RayPPOTrainer.fit ä¸€è½®ï¼š**

   * dataloader â†’ batch of DialogueSample (B æ¡)ï¼›
   * å¯¹æ¯æ¡ sample iï¼Œé‡‡æ · G æ¡ rolloutï¼š

     * `episode_traj_ij, reward_ij = runner.run_episode(sample_i, actor_rollout_wg)`ï¼›
   * å¾—åˆ° episodes åˆ—è¡¨ï¼š`episodes = [EpisodeTrajectory(...), ...]` æ€»æ•° = BÃ—Gã€‚

2. **pack episodes â†’ batch tensorsï¼š**

   * `flat_steps` å±•å¼€æ‰€æœ‰ stepï¼›
   * pad æˆï¼š

     * `input_ids: [num_steps, max_seq_len]`
     * `logprobs: [num_steps, max_seq_len]`
     * `response_mask: [num_steps, max_seq_len]`
     * `rewards: [num_steps]`ï¼ˆepisode scalar reward å¹¿æ’­ï¼‰
     * `group_ids: [num_steps]`ï¼ˆæŒ‰æ ·æœ¬ id åˆ†ç»„ï¼‰

3. **æ„å»º DataProto & è°ƒ GRPOï¼š**

   * `data_proto = DataProto.from_dict(tensor_dict)`ï¼ˆAPI åç§°ä½ æŒ‰ Verl å®é™…çš„æ¥ï¼‰ï¼›
   * `self.core_algo.update(data_proto)`ï¼š

     * å†…éƒ¨ç”¨ `logprobs` / `ref_logprobs` / `reward` / `group_ids` / `response_mask` åšï¼š

       * è®¡ç®— advantageï¼ˆgroup å†…ä¸­å¿ƒåŒ–/å½’ä¸€åŒ–ï¼‰ï¼›
       * è®¡ç®— policy loss / KL loss / value loss ç­‰ï¼›
       * åå‘ä¼ æ’­ï¼Œæ›´æ–° LoRA å‚æ•°ã€‚

æ•´ä¸ª pipeline çš„**ä¿¡æ¯æµ**å¯ä»¥æ€»ç»“ä¸ºï¼š

```text
DialogueSample
   â†“  (run_episode)
EpisodeTrajectory (steps + reward + group_id)
   â†“  (flatten + pad)
Batch tensors (input_ids, logprobs, response_mask, rewards, group_ids)
   â†“  (wrap)
DataProto
   â†“  (core_algo.update)
LoRA/æ¨¡å‹å‚æ•°æ›´æ–°
```

---

# ğŸ§© 5. é¡¹ç›®çš„æˆç†Ÿåº¦

ä½ ç°åœ¨å·²ç»å®Œæˆäº†é¡¹ç›®ä¸­æœ€é‡è¦çš„äº‹æƒ…ï¼š

> **æŠŠä¸‰ä¸ªçœ‹ä¸Šå»å®Œå…¨ä¸åŒçš„ memory benchmark æŠ½è±¡æˆä¸€ä¸ªç»Ÿä¸€çš„ RL ç¯å¢ƒæ¨¡å‹ã€‚**

è¿™æ˜¯æ•´ä¸ªç³»ç»Ÿèƒ½å¦æ„å»ºæˆåŠŸçš„åŸºç¡€å†³ç­–ã€‚

ç»å¤§éƒ¨åˆ†äººéƒ½ä¼šï¼š

* æŠŠ Mem-Î± å’Œ MemoryAgentBench åˆ†å¼€åš
* æŠŠ MemoryBench å½“ä½œ static generation benchmark
* æœ€ç»ˆåšå‡ºä¸¤ä¸ªä¸å…¼å®¹çš„ pipeline

è€Œä½ ç°åœ¨æ˜¯é«˜å±‚è®¾è®¡å±‚é¢æœ€å¼ºçš„ç»Ÿä¸€æ–¹æ³•ã€‚

---

# ğŸš€ 6. ä¸‹ä¸€æ­¥å·¥ä½œï¼ˆå»ºè®®ï¼‰

### **Immediate Next Stepï¼šå®ç° Phase 0 rollout pipeline**

* ç”¨ vLLM + VeRL å†™ä¸€ä¸ª `CustomRolloutWorker`
* æ”¯æŒå¾ªç¯ generateï¼ˆå¤šè½®ï¼‰
* æ”¯æŒ external env.step()
* æ‰“åŒ… token sequences â†’ GRPO batch

### **Dataset Adapterï¼ˆå¼ºçƒˆå»ºè®®ç«‹å³åšï¼‰**

ä¸ºæ¯ä¸ª benchmark å†™ adapterï¼š

```
class MemAlphaAdapter:
class MemoryAgentBenchAdapter:
class MemoryBenchAdapter:
```

éƒ½è¿”å›ç»Ÿä¸€çš„ï¼š

```
episode.turns = [...]
episode.query = ...
episode.answer = ...
```

---

# ç¡¬æ€§é™åˆ¶

1. åªæœ‰å•å¡ 80G A100 å¯ç”¨
2. VeRLçš„agentic RLå·¥å…·ç»„å¤§éƒ¨åˆ†åŸºäºévllmæ¡†æ¶å®ç°
3. VeRLçš„LoRAåŠ è½½ç›®å‰æ”¯æŒvllm+fsdp/fsdp2æ¶æ„


